{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Monitor a Model Endpoint using SageMaker Model Monitor\n",
    "\n",
    "* Goals\n",
    "    * Create a baselining job to learn dataset contraints and statistics\n",
    "    * Enable data capture to store input data\n",
    "    * Compare input data to constraints and statistics from the training set\n",
    "    * Visualize differences in the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sagemaker-workshop-420/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /root/sagemaker-workshop-420/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::209970524256:role/service-role/AmazonSageMaker-ExecutionRole-20200402T065938\n"
     ]
    }
   ],
   "source": [
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artifacts will be written to/read from s3://sagemaker-workshop-420/xgb-churn\n"
     ]
    }
   ],
   "source": [
    "BUCKET = 'sagemaker-workshop-420'\n",
    "PREFIX = 'xgb-churn'\n",
    "\n",
    "LOCAL_DATA_DIRECTORY = f'../data/{PREFIX}'\n",
    "\n",
    "print(f\"\\nArtifacts will be written to/read from s3://{BUCKET}/{PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the pretrained XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_image_name = get_image_uri(boto_session.region_name, 'xgboost', repo_version='0.90-2')\n",
    "xgboost_image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_path = 's3://sagemaker-workshop-420/xgb-churn/output/xgboost-customer-churn-2020-04-11-12-15-49-863/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = Model(model_data=model_s3_path,\n",
    "                      image=xgboost_image_name,\n",
    "                      role=role,\n",
    "                      sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Host the Model and Enable Data Capture\n",
    "\n",
    "Now that we've trained the model, let's deploy it to a hosted endpoint. To monitor the model after it's hosted and serving requests, we'll also add configurations to capture data that is being sent to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import strftime, gmtime\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DatasetFormat, DefaultModelMonitor\n",
    "from sagemaker.predictor import csv_serializer, RealTimePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_config = DataCaptureConfig(enable_capture=True,\n",
    "                                        sampling_percentage=100,\n",
    "                                        destination_s3_uri=f's3://{BUCKET}/{PREFIX}/model-monitor/data-capture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName = xgboost-customer-churn-2020-04-12-20-57-13\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"xgboost-customer-churn-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"EndpointName = {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgboost_model.deploy(initial_instance_count=1, \n",
    "                                     instance_type='ml.m4.xlarge',\n",
    "                                     endpoint_name=endpoint_name,\n",
    "                                     data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-xgboost-2020-04-12-20-57-18-515'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-customer-churn-2020-04-12-20-57-13'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Initialize a Predictor \n",
    "\n",
    "Initialize a `sagemaker.predictor.RealTimePredictor` so we can make real-time predictions from our model by making an HTTP POST request. We also set up serializers and deserializers for passing our NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = RealTimePredictor(endpoint=xgboost_model.endpoint_name,\n",
    "                                  sagemaker_session=sagemaker_session,\n",
    "                                  serializer = csv_serializer,\n",
    "                                  deserializer=None,\n",
    "                                  content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Invoke the deployed model\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions. First let's load in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll loop over our test dataset and collect predictions by invoking the XGBoost endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186,0.1,137.8,97,187.7,118,146.4,85,8.7,6,1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10,0.11,0.12,0.13,0.14,0.15,0.16,0.17,1.1,0.18,0.19,0.20,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.30,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.40,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.50,0.51,0.52,0.53,1.2,1.3,0.54,1.4,0.55\n",
      "132,25,113.2,96,269.9,107,229.1,87,7.1,7,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1\n",
      "112,17,183.2,95,252.8,125,156.7,95,9.7,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1\n",
      "91,24,93.5,112,183.4,128,240.7,133,9.9,3,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1\n",
      "22,0,110.3,107,166.5,93,202.3,96,9.5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../data/xgb-churn/test_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint xgboost-customer-churn-2020-04-12-20-57-13. \n",
      "Please wait for a minute...\n"
     ]
    }
   ],
   "source": [
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait for a minute...\".format(endpoint_name))\n",
    "\n",
    "with open(f'{LOCAL_DATA_DIRECTORY}/test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        #print(payload)\n",
    "        #break\n",
    "        response = xgb_predictor.predict(data=payload)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that data is captured in Amazon S3\n",
    "\n",
    "When we made some real-time predictions by sending data to our endpoint, we should have also captured that data for monitoring purposes. \n",
    "\n",
    "Let's list the data capture files stored in Amazon S3. Expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgb-churn/datacapture/xgboost-customer-churn-2020-04-12-20-57-13'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_endpoint_capture_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Data Capture Files:\n",
      "['s3://sagemaker-workshop-420/xgb-churn/xgboost-customer-churn-2020-04-12-20-57-13/AllTraffic/2020/04/12/21/05-21-406-c96a56c4-8bc2-407d-88cc-2785f9e354e6.jsonl', 's3://sagemaker-workshop-420/xgb-churn/xgboost-customer-churn-2020-04-12-20-57-13/AllTraffic/2020/04/12/21/06-21-675-c2c2d0c4-f136-4954-aecd-66066e76b373.jsonl']\n"
     ]
    }
   ],
   "source": [
    "current_endpoint_capture_prefix = f'{PREFIX}/{endpoint_name}'\n",
    "print(\"Found Data Capture Files:\")\n",
    "capture_files = S3Downloader.list(f\"s3://{BUCKET}/{current_endpoint_capture_prefix}\")\n",
    "print(capture_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data captured is stored in a SageMaker specific json-line formatted file. Next, Let's take a quick peek at the contents of a single line in a pretty formatted json so that we can observe the format a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Single Data Capture====\n",
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"92,0,176.3,85,93.4,125,207.2,107,9.6,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"0.041860517114400864\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"eacd5b42-561a-46cf-bc82-99d8c21c4625\",\n",
      "    \"inferenceTime\": \"2020-04-12T21:06:21Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "capture_file = S3Downloader.read_file(capture_files[-1])\n",
    "\n",
    "print(\"=====Single Data Capture====\")\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon SageMaker Model Monitor\n",
    "\n",
    "Amazon SageMaker Model Monitor lets you monitor and evaluate the data observed by endpoints. It works like this:\n",
    "1. We need to create a baseline that we can use to compare real-time traffic against. \n",
    "1. When a baseline is ready, we can set up a schedule to continously evaluate and compare against the baseline.\n",
    "1. We can send synthetic traffic to trigger alarms.\n",
    "\n",
    "**Important**: It takes an hour or more to complete this section because the shortest monitoring polling time is one hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselining and continous monitoring\n",
    "\n",
    "#### 1. Constraint suggestion with the baseline (training) dataset\n",
    "\n",
    "The training dataset that you use to train a model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema must match exactly (for example, they should have the same number and type of features).\n",
    "\n",
    "Using our training dataset, let's ask Amazon SageMaker Model Monitor to suggest a set of baseline `constraints` and generate descriptive `statistics` so we can explore the data. For this example, let's upload the training dataset, which we used to train model. We'll use the dataset file with column headers so we have descriptive feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-workshop-420/xgb-churn/baselining/data\n",
      "Baseline results uri: s3://sagemaker-workshop-420/xgb-churn/baselining/results\n"
     ]
    }
   ],
   "source": [
    "baseline_prefix = PREFIX + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(BUCKET, baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(BUCKET, baseline_results_prefix)\n",
    "\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
    "\n",
    "baseline_data_path = S3Uploader.upload(f\"{LOCAL_DATA_DIRECTORY}/training-dataset-with-header.csv\", baseline_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a baselining job with the training dataset\n",
    "\n",
    "Now that we have the training data ready in S3, let's start a job to `suggest` constraints. To generate the constraints, the convenient helper starts a `ProcessingJob` using a ProcessingJob container provided by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor = DefaultModelMonitor(role=role,\n",
    "                                         instance_count=1,\n",
    "                                         instance_type='ml.m5.xlarge',\n",
    "                                         volume_size_in_gb=20,\n",
    "                                         max_runtime_in_seconds=3600)\n",
    "\n",
    "baseline_job = my_default_monitor.suggest_baseline(baseline_dataset=baseline_data_path,\n",
    "                                                   dataset_format=DatasetFormat.csv(header=True),\n",
    "                                                   output_s3_uri=baseline_results_uri,\n",
    "                                                   wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job succeeds, we can explore the `baseline_results_uri` location in s3 to see what files where stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found Files:\")\n",
    "S3Downloader.list(f\"s3://{BUCKET}/{baseline_results_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a`constraints.json` file that has information about suggested constraints. We also have a `statistics.json` which contains statistical information about the data in the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Analyzing subsequent captures for data quality issues\n",
    "\n",
    "Now that we've generated a baseline dataset and processed it to get baseline statistics and constraints, let's monitor and analyze the data being sent to the endpoint with monitoring schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a schedule\n",
    "\n",
    "First, let's create a monitoring schedule for the endpoint. The schedule specifies the cadence at which we want to run a new processing job so that we can compare recent data captures to the baseline. This schedule will apply to the Endpoint created before and also the baseline resources (constraints and statistics) which were generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "reports_prefix = f'{PREFIX}/reports'\n",
    "s3_report_path = f's3://{BUCKET}/{reports_prefix}'\n",
    "\n",
    "mon_schedule_name = 'xgboost-customer-churn-model-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(monitor_schedule_name=mon_schedule_name,\n",
    "                                              endpoint_input=xgb_predictor.endpoint,\n",
    "                                              output_s3_uri=s3_report_path,\n",
    "                                              statistics=my_default_monitor.baseline_statistics(),\n",
    "                                              constraints=my_default_monitor.suggested_constraints(),\n",
    "                                              schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "                                              enable_cloudwatch_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Start generating some artificial traffic\n",
    "The following block starts a thread to send some traffic to the endpoint. This allows us to continue to send traffic to the endpoint so that we'll have data continually captured for analysis. If there is no traffic, the monitoring jobs will start to fail later.\n",
    "\n",
    "To terminate this thread, you need to stop the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n",
    "                                                      ContentType='text/csv', \n",
    "                                                      Body=payload)\n",
    "            time.sleep(1)\n",
    "            \n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint_name, f'{LOCAL_DATA_DIRECTORY}/test-dataset-input-cols.csv', runtime_client)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "\n",
    "# Note that you need to stop the kernel to stop the invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List executions\n",
    "Once the schedule is set up, jobs start at the specified intervals. The following code lists the last five executions. If you run this code soon after creating the hourly schedule, you might not see any executions listed. To see executions, you might have to wait until you cross the hour boundary (in UTC). The code includes the logic for waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "if len(mon_executions) == 0:\n",
    "    print(\"We created a hourly schedule above and it will kick off executions ON the hour.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the latest execution and list the generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1]\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()['ExitMessage']))\n",
    "report_uri = latest_execution.output.destination\n",
    "\n",
    "print(\"Found Report Files:\")\n",
    "S3Downloader.list(report_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List violations\n",
    "\n",
    "If there are any violations compared to the baseline, they will be generated here. Let's list the violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Execution and Baseline details from Processing Job Arn\n",
    "\n",
    "Enter the ProcessingJob arn for an execution of a MonitoringSchedule below to get the result files associated with that execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:us-east-2:209970524256:processing-job/model-monitoring-202004111100-a71a88f315417d8d74d5f73f'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job_arn = latest_execution.describe()['ProcessingJobArn']\n",
    "processing_job_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sagemaker.model_monitor import MonitoringExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = MonitoringExecution.from_processing_arn(sagemaker_session=sagemaker_session, processing_job_arn=processing_job_arn)\n",
    "exec_inputs = {inp['InputName']: inp for inp in execution.describe()['ProcessingInputs']}\n",
    "exec_results = execution.output.destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_statistics_filepath = exec_inputs['baseline']['S3Input']['S3Uri'] if 'baseline' in exec_inputs else None\n",
    "execution_statistics_filepath = os.path.join(exec_results, 'statistics.json')\n",
    "violations_filepath = os.path.join(exec_results, 'constraint_violations.json')\n",
    "\n",
    "baseline_statistics = json.loads(S3Downloader.read_file(baseline_statistics_filepath)) if baseline_statistics_filepath is not None else None\n",
    "execution_statistics = json.loads(S3Downloader.read_file(execution_statistics_filepath))\n",
    "violations = json.loads(S3Downloader.read_file(violations_filepath))['violations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The code below shows the violations and constraichecks across all features in a simple table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.show_violation_df(baseline_statistics=baseline_statistics, latest_statistics=execution_statistics, violations=violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "\n",
    "This section visualizes the distribution and renders the distribution statistics for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mu.get_features(execution_statistics)\n",
    "feature_baselines = mu.get_features(baseline_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.show_distributions(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Stats vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.show_distributions(features, feature_baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "If you no longer need this notebook, clean up your environment by running the following cell. It removes the hosted endpoint that you created for this walkthrough and prevents you from incurring charges for running an instance that you no longer need. It also cleans up all artifacts related to the experiments. \n",
    "\n",
    "You'll also want to delete artifacts stored in the S3 bucket used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.delete_monitoring_schedule(mon_schedule_name)\n",
    "sagemaker_session.delete_endpoint(xgb_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
